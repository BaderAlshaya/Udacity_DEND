# Project 1: Data Modeling with Postgres


## Introduction
A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

They'd like us to create a Postgres database with tables designed to optimize queries on song play analysis. Our role is to create a database schema and ETL pipeline for this analysis. You'll be able to test your database and ETL pipeline by running queries given to you by the analytics team from Sparkify and compare our results with their expected results.
<br><br>


## Project Description
**Requirements:**
- We collect the data for user activity from a music streaming app called **Sparkify** in `JSON` file format.
- We creat a **Relational Database** to store the data using `PostgreSQL`. 
- We define **Fact** and **Dimension** tables for a **Star Schema** with a particular analytic focus. 
- We build an **ETL Pipeline** to transfer the date into these tables using `Python` and `SQL`.
- We optimize queries and understand what songs users are listening to.
- We finally test the database by comparing their given expected results with our own query results.
<br>


**Files:**
- (Python) **`create_tables.py`**: Set up the `sparkifydb` database and creates the tables.
- (Python) **`sql_queries.py`**: Specify insertion query template and initialize SQL queries to create and drope tables.
- (Python) **`etl.py`**: Read and transfer song_data and log_data.
- (Jupyter Notebook) **`etl.ipynb`**: Analyse dataset before loading it to the database.
- (Jupyter Notebook) **`test.ipynb`**: Test loaded data against expected data to validate the results.
- (Text) **`requirements.txt`**: Populate project's required packages.
<br>


**Data:**
- Song Dataset: 

Each song file is in `JSON` file format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.
<br>**For instance:** `song_data/A/A/B/TRAABJL12903CDCF1A.json` looks like:

    {"num_songs": 1, 
    "artist_id": "ARJIE2Y1187B994AB7", 
    "artist_latitude": null, 
    "artist_longitude": null,
    "artist_location": "", 
    "artist_name": "Line Renaud", 
    "song_id": "SOUPIRU12A6D4FA1E1", 
    "title": "Der Kleine Dompfaff", 
    "duration": 152.92036, 
    "year": 0}

- Log Dataset:

Each log file is in `JSON` file format generated by an event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations. The files are partitioned by year and month. 
<br>**For instance:** `log_data/2018/11/2018-11-10-events.json` looks like:

    {"artist":"Hoobastank",
    "auth":"Logged In",
    "firstName":"Cierra",
    "gender":"F",
    "itemInSession":0,
    "lastName":"Finley",
    "length":241.3971,
    "level":"free",
    "location":"Richmond, VA",
    "method":"PUT",
    "page":"NextSong",
    "registration":1541013292796.0,
    "sessionId":132,
    "song":"Say The Same",
    "status":200,
    "ts":1541808927796,
    "userAgent":"\"Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit\/537.77.4 (KHTML, like Gecko) Version\/7.0.5 Safari\/537.77.4\"",
    "userId":"96"}
<br>


**Database Schema Design:**

This project transfer the data from two data sets, `song_data` and `log_data`, as shown above. The data gets transfered from these JSON files using the ETL Pipline method. Which represent the following three states:
- *Extract:* where the data gets extracted from JSON files.
- *Transform:* where the data gets transformed into a transferable data type.
- *Load:* where the data gets loaded to the target database.

**The design for the database is the following Star Schema:**
<p align="center"> 
    <img alt="Star Schema Design"
    src="https://github.com/BaderAlshaya/Udacity_DEND/blob/master/p1/assets/images/StarSchemaDesign.png"> 
</p>
<br><br>


## Build
To build the project (Mac OS):
- Install Python 3 (or Above).
- Install, create, and activate Python 3 virtual environment ([venv](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/)).
- Install all required packages and their specific versions using `pip3 install -r requirements.txt`.
- Install postgres (PostgreSQL 12.3 or above).
- Open the terminal and run the following commands to create and run the databases locally:
        **<br>- Run the PostgresSQL shell:** `psql`
        **<br>- Create the database owner:** `CREATE ROLE student WITH LOGIN PASSWORD 'student' CREATEDB CREATEROLE;`
        **<br>- Quit the PostgresSQL shell:** `\q`
        **<br>- Create a default database:** `createdb -O student -h 127.0.0.1 -e studentdb`
<br><br>


## Run
To run the project:
- Run python3 create_tables.py 
- Run python3 etl.py
<br><br>


## Test Result
<br><br>


- Discuss the purpose of this database in the context of the startup, Sparkify, and their analytical goals.
- State and justify your database schema design and ETL pipeline.
- [Optional] Provide example queries and results for song play analysis.

