# Project 1: Data Modeling with Postgres


## Introduction
A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

They'd like us to create a Postgres database with tables designed to optimize queries on song play analysis. Our role is to create a database schema and ETL pipeline for this analysis. You'll be able to test your database and ETL pipeline by running queries given to you by the analytics team from Sparkify and compare our results with their expected results.


## Project Description
**Requirements:**
- We collect the data for user activity from a music streaming app called **Sparkify** in `JSON` file format.
- We creat a **Relational Database** to store the data using `PostgreSQL`. 
- We define **Fact** and **Dimension** tables for a **Star Schema** with a particular analytic focus. 
- We build an **ETL Pipeline** to transfer the date into these tables using `Python` and `SQL`.
- We optimize queries and understand what songs users are listening to.
- We finally test the database by comparing their given expected results with our own query results.




**Files:**
- (Text) **`requirements.txt`**: Populate project's required packages.
- (Python) **`create_tables.py`**: Connect to the database and create the tables.
- (Python) **`sql_queries.py`**: Sets the tables and the queries used for the ETL Pipline.
- (Python) **`etl.py`**: Transfers the data from the JSON files to the target database.
- (Jupyter Notebook) **`etl.ipynb`**: Analyse dataset before loading it to the database.
- (Jupyter Notebook) **`test.ipynb`**: Test loaded data against expected data to validate the transformation.



**Data:**
`Song Dataset`: 

Each song file is in `JSON` file format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.

**For instance:** `song_data/A/A/B/TRAABJL12903CDCF1A.json` looks like:

    {"num_songs": 1,
    "artist_id": "ARHHO3O1187B989413",
    "artist_latitude": null,
    "artist_longitude": null,
    "artist_location": "",
    "artist_name": "Bob Azzam",
    "song_id": "SORAMLE12AB017C8B0",
    "title": "Auguri Cha Cha",
    "duration": 191.84281,
    "year": 0}

**The Dataset After ETL Pipeline Transfer**

`Log Dataset`:

Each log file is in `JSON` file format generated by an event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations. The files are partitioned by year and month. 
**For instance:** `log_data/2018/11/2018-11-10-events.json` looks like:

    {"artist":"Hoobastank",
    "auth":"Logged In",
    "firstName":"Cierra",
    "gender":"F",
    "itemInSession":0,
    "lastName":"Finley",
    "length":241.3971,
    "level":"free",
    "location":"Richmond, VA",
    "method":"PUT",
    "page":"NextSong",
    "registration":1541013292796.0,
    "sessionId":132,
    "song":"Say The Same",
    "status":200,
    "ts":1541808927796,
    "userAgent":"\"Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit\/537.77.4 (KHTML, like Gecko) Version\/7.0.5 Safari\/537.77.4\"",
    "userId":"96"}



**Database Schema Design:**

Using the **ETL Pipline** method, the data gets transferred from two data sets `song_data` and `log_data` (as shown above). ETL stands for:
- *Extract:* where the data gets extracted from JSON files.
- *Transform:* where the data gets transformed into a transferable data type.
- *Load:* where the data gets loaded to the target database.

**The design for the database is the following Star Schema:**


![Star Schema Design](https://github.com/BaderAlshaya/Udacity_DEND/blob/master/p1/assets/images/StarSchemaDesign.png?raw=true)



## Build
To build the project (Mac OS):
- Install Python 3 (or Above).
- Install, create, and activate Python 3 virtual environment ([venv](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/)).
- Install all required packages and their specific versions using `pip3 install -r requirements.txt`.
- Install postgres (PostgreSQL 12.3 or above).
- Open the terminal and run the following commands to create and run the databases locally:

    &nbsp;&nbsp;&nbsp;**- Run the PostgresSQL shell:** `psql`
    &nbsp;&nbsp;&nbsp;**- Create the database owner:** `CREATE ROLE student WITH LOGIN PASSWORD 'student' CREATEDB CREATEROLE;`
    &nbsp;&nbsp;&nbsp;**- Quit the PostgresSQL shell:** `\q`
    &nbsp;&nbsp;&nbsp;**- Create a default database:** `createdb -O student -h 127.0.0.1 -e studentdb`



## Run
Follow the steps below to compile and run the project with Terminal:
- Run `python3 create_tables.py `
- Run `python3 etl.py`



## Test Result
The test results are provided in the following Jupyter notebook files:
- `etl.ipynb`
- `test.ipynb`

